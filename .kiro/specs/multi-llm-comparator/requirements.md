# 需求文档

## 介绍

本地部署的多LLM模型比较器是一个基于Python 3.12和uv包管理器开发的Streamlit Web应用，允许用户同时比较多个大语言模型的输出结果。该系统支持PyTorch和GGUF格式的本地模型，最多可同时比较4个模型，采用内存优化策略确保资源合理使用，并提供流式输出和Markdown格式的美观展示。项目使用uv进行依赖管理和虚拟环境管理，确保开发和部署的一致性。

## 需求

### 需求 1 - 项目环境和依赖管理

**用户故事：** 作为开发者，我希望使用uv包管理器来管理Python 3.12项目的依赖和虚拟环境，以便确保开发和部署环境的一致性。

#### 验收标准

1. 当初始化项目时，系统应当使用uv创建Python 3.12的虚拟环境
2. 当安装依赖时，系统应当使用uv管理所有Python包的安装和版本控制
3. 当运行应用时，系统应当在uv管理的虚拟环境中执行
4. 当部署应用时，系统应当提供基于uv的依赖锁定文件确保版本一致性
5. 当开发者设置环境时，系统应当提供清晰的uv环境配置说明

### 需求 2 - 本地模型发现与类型识别

**用户故事：** 作为用户，我希望系统能够自动发现并识别本地存储的LLM模型文件，以便我可以方便地选择要比较的模型。

#### 验收标准

1. 当系统启动时，系统应当扫描指定目录中的模型文件
2. 当系统扫描模型文件时，系统应当自动识别PyTorch模型(.bin, .pt, .pth, .safetensors文件)
3. 当系统扫描模型文件时，系统应当自动识别GGUF模型(.gguf文件)
4. 当系统识别到未支持的模型格式时，系统应当跳过该文件并记录警告
5. 当系统完成扫描后，系统应当在界面中显示可用模型列表及其类型标识

### 需求 3 - 模型选择与配置管理

**用户故事：** 作为用户，我希望能够选择最多4个模型进行比较，并为每个模型配置合适的推理参数。

#### 验收标准

1. 当用户在Streamlit界面中选择模型时，系统应当允许选择1到4个不同的模型
2. 如果用户尝试选择超过4个模型，系统应当显示错误提示并阻止选择
3. 当用户选择PyTorch模型时，系统应当提供temperature、top_p、max_tokens、do_sample等参数配置
4. 当用户选择GGUF模型时，系统应当提供temperature、top_k、top_p、repeat_penalty、n_ctx等参数配置
5. 当用户修改参数时，系统应当实时验证参数范围的有效性
6. 当用户保存配置时，系统应当将每个模型的参数配置持久化存储到本地JSON文件

### 需求 4 - 内存优化的模型加载策略

**用户故事：** 作为用户，我希望系统能够智能管理内存使用，确保在资源有限的环境中也能顺利运行多模型比较。

#### 验收标准

1. 当用户开始比较时，系统应当按顺序逐个加载模型而非同时加载所有模型
2. 当系统加载PyTorch模型时，系统应当使用transformers库和torch进行加载和推理
3. 当系统加载GGUF模型时，系统应当使用llama-cpp-python库进行加载和推理
4. 当一个模型完成推理后，系统应当立即释放该模型占用的GPU/CPU内存
5. 当系统处理下一个模型前，系统应当确保前一个模型已完全卸载并执行垃圾回收
6. 当所有模型处理完成时，系统应当清理所有相关的内存占用并显示内存使用统计

### 需求 5 - 流式输出与实时显示

**用户故事：** 作为用户，我希望能够实时看到每个模型的生成过程，而不是等待所有模型都完成后才看到结果。

#### 验收标准

1. 当模型开始生成内容时，系统应当在Streamlit界面中立即显示对应的输出区域
2. 当模型生成新的token时，系统应当使用Streamlit的st.empty()和实时更新功能立即显示新内容
3. 当模型生成过程中，系统应当显示当前生成进度、已用时间和预估剩余时间
4. 当模型生成完成时，系统应当显示完成状态、总用时、生成的token数量和平均生成速度
5. 如果模型生成过程中出现错误，系统应当显示具体错误信息并继续处理其他模型

### 需求 6 - Markdown渲染与格式化显示

**用户故事：** 作为用户，我希望模型输出的内容能够以美观、易读的格式展示，特别是包含代码和结构化内容时。

#### 验收标准

1. 当模型输出包含Markdown语法时，系统应当使用Streamlit的st.markdown()组件正确渲染
2. 当输出包含代码块时，系统应当提供语法高亮显示并支持代码复制功能
3. 当输出包含表格、列表等结构时，系统应当保持正确的格式化显示
4. 当用户需要时，系统应当提供切换查看渲染视图和原始文本的选项
5. 当内容较长时，系统应当提供滚动查看功能和内容折叠展开选项

### 需求 7 - 比较界面与结果导出

**用户故事：** 作为用户，我希望能够在统一的界面中方便地比较不同模型的输出，并能够保存比较结果。

#### 验收标准

1. 当用户输入提示词时，系统应当通过Streamlit的st.text_area()组件接收多行用户输入
2. 当开始比较时，系统应当使用Streamlit的st.columns()布局并排显示各模型的输出区域
3. 当所有模型完成生成时，系统应当显示每个模型的性能统计信息（用时、token数、速度等）
4. 当用户需要保存结果时，系统应当提供导出功能，支持JSON、CSV、Markdown、TXT等格式
5. 当用户需要重新比较时，系统应当提供清除当前结果并开始新比较的功能
6. 当界面显示多个模型输出时，系统应当为每个输出区域提供清晰的模型标识和状态指示器