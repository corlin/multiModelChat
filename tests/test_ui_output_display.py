"""
UIè¾“å‡ºæ˜¾ç¤ºåŠŸèƒ½å•å…ƒæµ‹è¯•
"""

import pytest
from unittest.mock import Mock, patch, MagicMock
from src.multi_llm_comparator.core.models import ModelInfo, ModelType


class TestOutputDisplayComponents:
    """è¾“å‡ºæ˜¾ç¤ºç»„ä»¶æµ‹è¯•"""
    
    @patch('streamlit.markdown')
    @patch('streamlit.progress')
    @patch('streamlit.text')
    @patch('streamlit.info')
    @patch('streamlit.error')
    def test_render_model_output_card_all_states(self, mock_error, mock_info, mock_text, 
                                               mock_progress, mock_markdown):
        """æµ‹è¯•æ¨¡å‹è¾“å‡ºå¡ç‰‡çš„æ‰€æœ‰çŠ¶æ€"""
        from src.multi_llm_comparator.ui.components import render_model_output_card
        
        # æµ‹è¯•ç­‰å¾…çŠ¶æ€
        render_model_output_card("Test Model", "pytorch", "pending")
        mock_info.assert_called_with("â³ ç­‰å¾…å¼€å§‹...")
        
        # æµ‹è¯•è¿è¡ŒçŠ¶æ€
        render_model_output_card("Test Model", "pytorch", "running")
        mock_progress.assert_called_with(0.5)
        mock_text.assert_called_with("ğŸ”„ æ­£åœ¨ç”Ÿæˆå›ç­”...")
        
        # æµ‹è¯•å®ŒæˆçŠ¶æ€
        content = "è¿™æ˜¯æµ‹è¯•å†…å®¹"
        stats = {'duration': 5.0, 'token_count': 100}
        render_model_output_card("Test Model", "pytorch", "completed", 
                               content=content, stats=stats)
        mock_markdown.assert_called_with(content)
        
        # æµ‹è¯•é”™è¯¯çŠ¶æ€
        error_msg = "æµ‹è¯•é”™è¯¯"
        render_model_output_card("Test Model", "pytorch", "error", error=error_msg)
        mock_error.assert_called_with(f"âŒ é”™è¯¯: {error_msg}")
    
    @patch('streamlit.text_area')
    def test_render_model_output_card_raw_view(self, mock_text_area):
        """æµ‹è¯•åŸå§‹æ–‡æœ¬è§†å›¾"""
        from src.multi_llm_comparator.ui.components import render_model_output_card
        
        content = "åŸå§‹æ–‡æœ¬å†…å®¹"
        render_model_output_card("Test Model", "pytorch", "completed", 
                               content=content, show_raw=True)
        
        mock_text_area.assert_called_once()
        call_args = mock_text_area.call_args
        assert call_args[1]['value'] == content
        assert call_args[1]['disabled'] is True
    
    @patch('streamlit.session_state', {})
    @patch('streamlit.empty')
    @patch('streamlit.markdown')
    @patch('streamlit.text')
    def test_render_streaming_output(self, mock_text, mock_markdown, mock_empty):
        """æµ‹è¯•æµå¼è¾“å‡ºæ¸²æŸ“"""
        from src.multi_llm_comparator.ui.components import render_streaming_output
        import streamlit as st
        
        # æ¨¡æ‹Ÿemptyå®¹å™¨
        mock_container = MagicMock()
        mock_empty.return_value = mock_container
        
        # æµ‹è¯•é¦–æ¬¡æ¸²æŸ“
        render_streaming_output("test_key", "æµ‹è¯•å†…å®¹", False)
        
        # éªŒè¯å®¹å™¨è¢«åˆ›å»º
        mock_empty.assert_called_once()
        
        # æµ‹è¯•å®ŒæˆçŠ¶æ€
        render_streaming_output("test_key", "å®Œæ•´å†…å®¹", True)
    
    @patch('streamlit.columns')
    def test_render_responsive_columns_different_counts(self, mock_columns):
        """æµ‹è¯•å“åº”å¼åˆ—å¸ƒå±€çš„ä¸åŒæ•°é‡"""
        from src.multi_llm_comparator.ui.components import render_responsive_columns
        
        # åˆ›å»ºåŠ¨æ€è¿”å›æ­£ç¡®æ•°é‡åˆ—çš„mockå‡½æ•°
        def mock_columns_func(num_cols):
            return [MagicMock() for _ in range(num_cols)]
        
        mock_columns.side_effect = mock_columns_func
        
        # æµ‹è¯•ä¸åŒæ•°é‡çš„é¡¹ç›®
        items = ["item1"]
        render_func = Mock()
        
        # 1ä¸ªé¡¹ç›®
        render_responsive_columns(items, render_func)
        mock_columns.assert_called_with(1)
        
        # 2ä¸ªé¡¹ç›®
        items = ["item1", "item2"]
        render_responsive_columns(items, render_func)
        mock_columns.assert_called_with(2)
        
        # 3ä¸ªé¡¹ç›®
        items = ["item1", "item2", "item3"]
        render_responsive_columns(items, render_func)
        mock_columns.assert_called_with(3)
        
        # 4ä¸ªé¡¹ç›®ï¼ˆåº”è¯¥ä½¿ç”¨2åˆ—ï¼Œè°ƒç”¨ä¸¤æ¬¡ï¼‰
        items = ["item1", "item2", "item3", "item4"]
        render_responsive_columns(items, render_func)
        # 4ä¸ªé¡¹ç›®æ—¶ä¼šè°ƒç”¨ä¸¤æ¬¡st.columns(2)
        assert mock_columns.call_args_list[-1] == ((2,),)
        assert mock_columns.call_args_list[-2] == ((2,),)
    
    def test_render_responsive_columns_empty_items(self):
        """æµ‹è¯•ç©ºé¡¹ç›®åˆ—è¡¨"""
        from src.multi_llm_comparator.ui.components import render_responsive_columns
        
        render_func = Mock()
        render_responsive_columns([], render_func)
        
        # éªŒè¯æ¸²æŸ“å‡½æ•°æ²¡æœ‰è¢«è°ƒç”¨
        render_func.assert_not_called()


class TestOutputDisplayIntegration:
    """è¾“å‡ºæ˜¾ç¤ºé›†æˆæµ‹è¯•"""
    
    def test_model_output_container_initialization(self):
        """æµ‹è¯•æ¨¡å‹è¾“å‡ºå®¹å™¨åˆå§‹åŒ–"""
        # è¿™ä¸ªæµ‹è¯•éœ€è¦æ¨¡æ‹ŸStreamlitä¼šè¯çŠ¶æ€
        from src.multi_llm_comparator.core.models import ModelInfo, ModelType
        
        models = [
            ModelInfo(
                id="model1",
                name="Test Model 1",
                path="/path/to/model1",
                model_type=ModelType.PYTORCH,
                size=1024,
                config={}
            ),
            ModelInfo(
                id="model2", 
                name="Test Model 2",
                path="/path/to/model2",
                model_type=ModelType.GGUF,
                size=2048,
                config={}
            )
        ]
        
        # æ¨¡æ‹Ÿåˆå§‹åŒ–è¿‡ç¨‹
        output_containers = {}
        output_status = {}
        
        for model in models:
            output_containers[model.id] = {
                'content': "",
                'status': "pending",
                'start_time': None,
                'end_time': None,
                'token_count': 0,
                'error': None
            }
            output_status[model.id] = "pending"
        
        # éªŒè¯åˆå§‹åŒ–ç»“æœ
        assert len(output_containers) == 2
        assert output_containers["model1"]['status'] == "pending"
        assert output_containers["model2"]['status'] == "pending"
        assert output_status["model1"] == "pending"
        assert output_status["model2"] == "pending"
    
    def test_comparison_results_structure(self):
        """æµ‹è¯•æ¯”è¾ƒç»“æœæ•°æ®ç»“æ„"""
        # æ¨¡æ‹Ÿæ¯”è¾ƒç»“æœ
        comparison_results = {
            'model1': {
                'model_name': 'Test Model 1',
                'model_type': 'pytorch',
                'status': 'completed',
                'content': 'è¿™æ˜¯æµ‹è¯•å›ç­”',
                'error': None,
                'stats': {
                    'start_time': 1000.0,
                    'end_time': 1005.0,
                    'token_count': 50,
                    'duration': 5.0
                }
            },
            'model2': {
                'model_name': 'Test Model 2',
                'model_type': 'gguf',
                'status': 'error',
                'content': '',
                'error': 'æ¨¡å‹åŠ è½½å¤±è´¥',
                'stats': None
            }
        }
        
        # éªŒè¯æ•°æ®ç»“æ„
        assert len(comparison_results) == 2
        
        # éªŒè¯æˆåŠŸçš„ç»“æœ
        model1_result = comparison_results['model1']
        assert model1_result['status'] == 'completed'
        assert model1_result['content'] != ''
        assert model1_result['error'] is None
        assert model1_result['stats'] is not None
        assert model1_result['stats']['duration'] == 5.0
        
        # éªŒè¯é”™è¯¯çš„ç»“æœ
        model2_result = comparison_results['model2']
        assert model2_result['status'] == 'error'
        assert model2_result['content'] == ''
        assert model2_result['error'] is not None
        assert model2_result['stats'] is None
    
    def test_output_status_transitions(self):
        """æµ‹è¯•è¾“å‡ºçŠ¶æ€è½¬æ¢"""
        # æ¨¡æ‹ŸçŠ¶æ€è½¬æ¢è¿‡ç¨‹
        status_transitions = [
            'pending',
            'running', 
            'completed'
        ]
        
        current_status = 'pending'
        
        # éªŒè¯çŠ¶æ€è½¬æ¢
        for next_status in status_transitions[1:]:
            assert current_status != next_status
            current_status = next_status
        
        assert current_status == 'completed'
        
        # æµ‹è¯•é”™è¯¯çŠ¶æ€
        error_status = 'error'
        assert error_status not in status_transitions
    
    def test_performance_stats_calculation(self):
        """æµ‹è¯•æ€§èƒ½ç»Ÿè®¡è®¡ç®—"""
        import time
        
        # æ¨¡æ‹Ÿç»Ÿè®¡æ•°æ®
        start_time = time.time()
        end_time = start_time + 5.0
        token_count = 100
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        duration = end_time - start_time
        tokens_per_second = token_count / duration if duration > 0 else 0
        
        stats = {
            'start_time': start_time,
            'end_time': end_time,
            'token_count': token_count,
            'duration': duration,
            'tokens_per_second': tokens_per_second
        }
        
        # éªŒè¯ç»Ÿè®¡ä¿¡æ¯
        assert stats['duration'] == 5.0
        assert stats['tokens_per_second'] == 20.0
        assert stats['token_count'] == 100
        
        # æµ‹è¯•é›¶é™¤æ³•ä¿æŠ¤
        zero_duration_stats = {
            'duration': 0,
            'token_count': 100
        }
        
        tps = zero_duration_stats['token_count'] / zero_duration_stats['duration'] if zero_duration_stats['duration'] > 0 else 0
        assert tps == 0


class TestOutputDisplayErrorHandling:
    """è¾“å‡ºæ˜¾ç¤ºé”™è¯¯å¤„ç†æµ‹è¯•"""
    
    def test_handle_empty_content(self):
        """æµ‹è¯•å¤„ç†ç©ºå†…å®¹"""
        from src.multi_llm_comparator.ui.components import render_model_output_card
        
        with patch('streamlit.info') as mock_info:
            render_model_output_card("Test Model", "pytorch", "pending", content="")
            mock_info.assert_called_with("â³ ç­‰å¾…å¼€å§‹...")
    
    def test_handle_invalid_status(self):
        """æµ‹è¯•å¤„ç†æ— æ•ˆçŠ¶æ€"""
        from src.multi_llm_comparator.ui.components import render_model_output_card
        
        with patch('streamlit.markdown') as mock_markdown:
            # æ— æ•ˆçŠ¶æ€åº”è¯¥ä½¿ç”¨é»˜è®¤é…ç½®
            render_model_output_card("Test Model", "pytorch", "invalid_status")
            
            # éªŒè¯markdownè¢«è°ƒç”¨ï¼ˆç”¨äºæ¸²æŸ“å¡ç‰‡å¤´éƒ¨ï¼‰
            mock_markdown.assert_called()
    
    def test_handle_missing_stats(self):
        """æµ‹è¯•å¤„ç†ç¼ºå¤±çš„ç»Ÿè®¡ä¿¡æ¯"""
        from src.multi_llm_comparator.ui.components import render_model_output_card
        
        with patch('streamlit.markdown'):
            # æ²¡æœ‰ç»Ÿè®¡ä¿¡æ¯æ—¶ä¸åº”è¯¥å´©æºƒ
            render_model_output_card("Test Model", "pytorch", "completed", 
                                   content="æµ‹è¯•å†…å®¹", stats=None)
    
    def test_handle_malformed_stats(self):
        """æµ‹è¯•å¤„ç†æ ¼å¼é”™è¯¯çš„ç»Ÿè®¡ä¿¡æ¯"""
        from src.multi_llm_comparator.ui.components import render_model_output_card
        
        malformed_stats = {
            'duration': 'invalid',  # åº”è¯¥æ˜¯æ•°å­—
            'token_count': None,    # åº”è¯¥æ˜¯æ•°å­—
        }
        
        with patch('streamlit.markdown'), patch('streamlit.expander') as mock_expander:
            mock_expander.return_value = MagicMock()
            
            # åº”è¯¥èƒ½å¤„ç†æ ¼å¼é”™è¯¯çš„ç»Ÿè®¡ä¿¡æ¯è€Œä¸å´©æºƒ
            render_model_output_card("Test Model", "pytorch", "completed",
                                   content="æµ‹è¯•å†…å®¹", stats=malformed_stats)


if __name__ == "__main__":
    pytest.main([__file__])