"""
Streamlitåº”ç”¨å…¥å£ç‚¹

å¤šLLMæ¨¡å‹æ¯”è¾ƒå™¨çš„ä¸»åº”ç”¨ç¨‹åºã€‚
"""

import streamlit as st
import os
import logging
import time
from pathlib import Path
from typing import Dict, Any, List

import sys
from pathlib import Path

# Add the src directory to the Python path
src_path = Path(__file__).parent.parent
sys.path.insert(0, str(src_path))

from multi_llm_comparator.services.model_manager import ModelManager
from multi_llm_comparator.services.inference_engine import InferenceEngine
from multi_llm_comparator.core.config import ConfigManager
from multi_llm_comparator.core.models import ModelType, ModelConfig
from multi_llm_comparator.core.exceptions import ModelSelectionError, ModelNotFoundError, ValidationError
from multi_llm_comparator.services.error_handler import handle_error, setup_error_handling
from multi_llm_comparator.services.recovery_manager import get_recovery_manager
from multi_llm_comparator.ui.enhancements import (
    initialize_ui_enhancements, show_enhanced_progress, show_operation_confirmation,
    render_undo_redo_controls, save_operation_state, render_loading_overlay,
    show_toast_notification, NotificationType, keyboard_manager, render_status_indicator,
    create_interactive_tutorial, render_advanced_text_viewer
)


# é…ç½®æ—¥å¿— - è®¾ç½®ä¸ºWARNINGçº§åˆ«ä»¥å‡å°‘å†—ä½™è¾“å‡º
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)


def initialize_session_state():
    """åˆå§‹åŒ–Streamlitä¼šè¯çŠ¶æ€"""
    # è®¾ç½®é”™è¯¯å¤„ç†
    if 'error_handling_setup' not in st.session_state:
        setup_error_handling("logs/streamlit_error.log")
        st.session_state.error_handling_setup = True
    
    # åˆå§‹åŒ–UIå¢å¼ºåŠŸèƒ½
    if 'ui_enhancements_initialized' not in st.session_state:
        initialize_ui_enhancements()
        st.session_state.ui_enhancements_initialized = True
    
    if 'model_manager' not in st.session_state:
        st.session_state.model_manager = ModelManager()
    
    if 'config_manager' not in st.session_state:
        st.session_state.config_manager = ConfigManager()
    
    if 'inference_engine' not in st.session_state:
        st.session_state.inference_engine = InferenceEngine()
    
    if 'recovery_manager' not in st.session_state:
        st.session_state.recovery_manager = get_recovery_manager()
    
    if 'models_scanned' not in st.session_state:
        st.session_state.models_scanned = False
    
    if 'comparison_running' not in st.session_state:
        st.session_state.comparison_running = False
    
    if 'comparison_results' not in st.session_state:
        st.session_state.comparison_results = {}
    
    if 'error_messages' not in st.session_state:
        st.session_state.error_messages = []
    
    # åˆå§‹åŒ–æµå¼è¾“å‡ºæ€§èƒ½ç›‘æ§
    if 'streaming_performance' not in st.session_state:
        st.session_state.streaming_performance = {
            'updates_processed': 0,
            'ui_refreshes': 0,
            'last_refresh_time': time.time(),
            'refresh_intervals': []
        }


def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ """
    with st.sidebar:
        st.header("ğŸ”§ é…ç½®")
        
        # æ¨¡å‹ç›®å½•é…ç½®
        st.subheader("æ¨¡å‹ç›®å½•")
        
        # é»˜è®¤æ¨¡å‹ç›®å½•
        default_dirs = ["models/pytorch", "models/gguf"]
        model_dirs = st.text_area(
            "æ¨¡å‹ç›®å½•ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰",
            value="\n".join(default_dirs),
            help="è¾“å…¥è¦æ‰«æçš„æ¨¡å‹ç›®å½•è·¯å¾„ï¼Œæ¯è¡Œä¸€ä¸ª"
        )
        
        # æ‰«æé€‰é¡¹
        recursive_scan = st.checkbox("é€’å½’æ‰«æå­ç›®å½•", value=True)
        
        # æ‰«ææŒ‰é’®
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ” æ‰«ææ¨¡å‹", use_container_width=True):
                scan_models(model_dirs.strip().split('\n'), recursive_scan)
        
        with col2:
            if st.button("ğŸ”„ åˆ·æ–°", use_container_width=True):
                refresh_models(model_dirs.strip().split('\n'), recursive_scan)
        
        # æ˜¾ç¤ºæ‰«æçŠ¶æ€
        if st.session_state.models_scanned:
            stats = st.session_state.model_manager.get_model_statistics()
            st.success(f"âœ… å‘ç° {stats['total_models']} ä¸ªæ¨¡å‹")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            with st.expander("ğŸ“Š æ¨¡å‹ç»Ÿè®¡"):
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("PyTorchæ¨¡å‹", stats['pytorch_models'])
                    st.metric("GGUFæ¨¡å‹", stats['gguf_models'])
                with col2:
                    st.metric("APIæ¨¡å‹", stats.get('api_models', 0))
                    st.metric("æ€»å¤§å°", f"{stats['total_size_gb']:.2f} GB")
        
        # æ˜¾ç¤ºæµå¼è¾“å‡ºæ€§èƒ½ç›‘æ§ï¼ˆä»…åœ¨æœ‰æ´»åŠ¨æ—¶æ˜¾ç¤ºï¼‰
        if st.session_state.get('comparison_running', False) or st.session_state.get('streaming_mode', False):
            perf = st.session_state.get('streaming_performance', {})
            if perf.get('ui_refreshes', 0) > 0:
                with st.expander("âš¡ æµå¼æ€§èƒ½ç›‘æ§"):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("UIåˆ·æ–°æ¬¡æ•°", perf.get('ui_refreshes', 0))
                        st.metric("å¤„ç†æ›´æ–°æ•°", perf.get('updates_processed', 0))
                    with col2:
                        intervals = perf.get('refresh_intervals', [])
                        if intervals:
                            avg_interval = sum(intervals) / len(intervals)
                            refresh_rate = 1 / avg_interval if avg_interval > 0 else 0
                            st.metric("å¹³å‡åˆ·æ–°ç‡", f"{refresh_rate:.1f} Hz")
                            st.metric("å¹³å‡é—´éš”", f"{avg_interval*1000:.0f} ms")
        
        # APIæ¨¡å‹ç®¡ç†
        st.divider()
        st.subheader("ğŸŒ APIæ¨¡å‹")
        
        with st.expander("â• æ·»åŠ Doubaoæ¨¡å‹"):
            with st.form("add_doubao_model"):
                st.write("æ·»åŠ æ–°çš„Doubaoæ¨¡å‹")
                
                doubao_model_id = st.text_input(
                    "æ¨¡å‹ID",
                    value="doubao-seed-1-6-250615",
                    help="Doubaoæ¨¡å‹çš„IDï¼Œå¦‚doubao-seed-1-6-250615"
                )
                
                doubao_display_name = st.text_input(
                    "æ˜¾ç¤ºåç§°",
                    value="",
                    help="åœ¨ç•Œé¢ä¸­æ˜¾ç¤ºçš„åç§°"
                )
                
                doubao_api_key = st.text_input(
                    "API Key (å¯é€‰)",
                    value="",
                    type="password",
                    help="ç•™ç©ºåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡ARK_API_KEY"
                )
                
                doubao_base_url = st.text_input(
                    "Base URL (å¯é€‰)",
                    value="https://ark.cn-beijing.volces.com/api/v3",
                    help="APIåŸºç¡€URL"
                )
                
                if st.form_submit_button("æ·»åŠ æ¨¡å‹"):
                    try:
                        model_info = st.session_state.model_manager.add_doubao_model(
                            model_id=doubao_model_id,
                            model_name=doubao_model_id.split('-')[-1] if '-' in doubao_model_id else doubao_model_id,
                            display_name=doubao_display_name or f"Doubao {doubao_model_id}"
                        )
                        st.success(f"âœ… å·²æ·»åŠ Doubaoæ¨¡å‹: {model_info.name}")
                        st.rerun()
                    except Exception as e:
                        st.error(f"æ·»åŠ æ¨¡å‹å¤±è´¥: {str(e)}")
        
        # æ˜¾ç¤ºå·²æ·»åŠ çš„APIæ¨¡å‹
        api_models = st.session_state.model_manager.api_manager.get_api_models()
        if api_models:
            with st.expander("ğŸ“‹ å·²é…ç½®çš„APIæ¨¡å‹"):
                for model in api_models:
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        st.text(f"ğŸŒ {model.name}")
                        st.caption(f"ID: {model.path}")
                    with col2:
                        if st.button("ğŸ—‘ï¸", key=f"remove_{model.id}", help="åˆ é™¤æ¨¡å‹"):
                            st.session_state.model_manager.api_manager.remove_model(model.id)
                            st.rerun()


def scan_models(directories: List[str], recursive: bool = True):
    """æ‰«ææ¨¡å‹"""
    try:
        # ä¿å­˜æ“ä½œçŠ¶æ€
        save_operation_state("æ‰«ææ¨¡å‹å‰")
        
        # ä½¿ç”¨å¢å¼ºçš„åŠ è½½æŒ‡ç¤ºå™¨
        render_loading_overlay("æ­£åœ¨æ‰«ææ¨¡å‹ç›®å½•...", show_progress=True, progress=0.3)
        
        # è¿‡æ»¤ç©ºç›®å½•
        valid_dirs = [d.strip() for d in directories if d.strip()]
        
        if not valid_dirs:
            show_toast_notification("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªæœ‰æ•ˆçš„ç›®å½•è·¯å¾„", NotificationType.ERROR)
            return
        
        # æ‰§è¡Œæ‰«æ
        result = st.session_state.model_manager.scan_models(valid_dirs, recursive)
        
        st.session_state.models_scanned = True
        
        # æ˜¾ç¤ºæ‰«æç»“æœ
        if result.valid_models > 0:
            show_toast_notification(f"æ‰«æå®Œæˆï¼å‘ç° {result.valid_models} ä¸ªæœ‰æ•ˆæ¨¡å‹", NotificationType.SUCCESS)
        else:
            show_toast_notification("æœªå‘ç°ä»»ä½•æœ‰æ•ˆæ¨¡å‹", NotificationType.WARNING)
        
        # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
        if result.errors:
            with st.expander("âš ï¸ æ‰«æè­¦å‘Š"):
                for error in result.errors:
                    st.warning(error)
    
    except Exception as e:
        # ä½¿ç”¨é”™è¯¯å¤„ç†å™¨å¤„ç†é”™è¯¯
        error_context = {
            'operation': 'scan_models',
            'directories': directories,
            'recursive': recursive
        }
        
        def show_error_callback(error_info):
            st.error(f"æ‰«æå¤±è´¥: {error_info.user_message}")
            
            # æ˜¾ç¤ºå»ºè®®
            if error_info.suggestions:
                with st.expander("ğŸ’¡ è§£å†³å»ºè®®"):
                    for suggestion in error_info.suggestions:
                        st.info(f"â€¢ {suggestion}")
        
        handle_error(e, error_context, show_error_callback)
        logger.error(f"æ¨¡å‹æ‰«æå¤±è´¥: {str(e)}")


def refresh_models(directories: List[str], recursive: bool = True):
    """åˆ·æ–°æ¨¡å‹åˆ—è¡¨"""
    try:
        # ä¿å­˜æ“ä½œçŠ¶æ€
        save_operation_state("åˆ·æ–°æ¨¡å‹å‰")
        
        render_loading_overlay("æ­£åœ¨åˆ·æ–°æ¨¡å‹åˆ—è¡¨...", show_progress=True, progress=0.5)
        
        valid_dirs = [d.strip() for d in directories if d.strip()]
        
        if not valid_dirs:
            show_toast_notification("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªæœ‰æ•ˆçš„ç›®å½•è·¯å¾„", NotificationType.ERROR)
            return
        
        result = st.session_state.model_manager.refresh_models(valid_dirs, recursive)
        st.session_state.models_scanned = True
        
        show_toast_notification(f"åˆ·æ–°å®Œæˆï¼å‘ç° {result.valid_models} ä¸ªæœ‰æ•ˆæ¨¡å‹", NotificationType.SUCCESS)
    
    except Exception as e:
        # ä½¿ç”¨é”™è¯¯å¤„ç†å™¨å¤„ç†é”™è¯¯
        error_context = {
            'operation': 'refresh_models',
            'directories': directories,
            'recursive': recursive
        }
        
        def show_error_callback(error_info):
            st.error(f"åˆ·æ–°å¤±è´¥: {error_info.user_message}")
            
            # æ˜¾ç¤ºå»ºè®®
            if error_info.suggestions:
                with st.expander("ğŸ’¡ è§£å†³å»ºè®®"):
                    for suggestion in error_info.suggestions:
                        st.info(f"â€¢ {suggestion}")
        
        handle_error(e, error_context, show_error_callback)
        logger.error(f"æ¨¡å‹åˆ·æ–°å¤±è´¥: {str(e)}")


def render_model_selection():
    """æ¸²æŸ“æ¨¡å‹é€‰æ‹©åŒºåŸŸ"""
    st.subheader("ğŸ“‹ æ¨¡å‹é€‰æ‹©")
    
    if not st.session_state.models_scanned:
        st.info("è¯·å…ˆåœ¨ä¾§è¾¹æ æ‰«ææ¨¡å‹ç›®å½•")
        return
    
    available_models = st.session_state.model_manager.get_available_models()
    
    if not available_models:
        st.warning("æœªå‘ç°ä»»ä½•å¯ç”¨æ¨¡å‹ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç›®å½•é…ç½®")
        return
    
    # æ¨¡å‹é€‰æ‹©ç•Œé¢
    selected_model_ids = st.session_state.model_manager.get_selected_model_ids()
    
    # åˆ›å»ºæ¨¡å‹é€‰æ‹©é€‰é¡¹
    model_options = {}
    for model in available_models:
        display_name = f"{model.name} ({model.model_type.value.upper()}) - {model.size / (1024*1024):.1f}MB"
        model_options[display_name] = model.id
    
    # å¤šé€‰æ¡†
    selected_displays = []
    for display_name, model_id in model_options.items():
        if model_id in selected_model_ids:
            selected_displays.append(display_name)
    
    new_selected_displays = st.multiselect(
        f"é€‰æ‹©æ¨¡å‹è¿›è¡Œæ¯”è¾ƒ (æœ€å¤š{st.session_state.model_manager.MAX_SELECTED_MODELS}ä¸ª)",
        options=list(model_options.keys()),
        default=selected_displays,
        help="é€‰æ‹©è¦è¿›è¡Œæ¯”è¾ƒçš„æ¨¡å‹"
    )
    
    # æ›´æ–°é€‰æ‹© - åªåœ¨é€‰æ‹©å®é™…æ”¹å˜æ—¶è°ƒç”¨
    new_selected_ids = [model_options[display] for display in new_selected_displays]
    current_selected_ids = st.session_state.model_manager.get_selected_model_ids()
    
    # æ£€æŸ¥é€‰æ‹©æ˜¯å¦å‘ç”Ÿäº†å˜åŒ–
    if set(new_selected_ids) != set(current_selected_ids):
        try:
            st.session_state.model_manager.select_models(new_selected_ids)
            
            # æ˜¾ç¤ºé€‰æ‹©çŠ¶æ€
            if new_selected_ids:
                st.success(f"å·²é€‰æ‹© {len(new_selected_ids)} ä¸ªæ¨¡å‹")
            
        except ModelSelectionError as e:
            st.error(str(e))
        except Exception as e:
            st.error(f"é€‰æ‹©æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
    elif new_selected_ids:
        # é€‰æ‹©æ²¡æœ‰å˜åŒ–ï¼Œä½†ä»æœ‰é€‰ä¸­çš„æ¨¡å‹ï¼Œæ˜¾ç¤ºå½“å‰çŠ¶æ€ï¼ˆä¸é‡å¤è°ƒç”¨select_modelsï¼‰
        st.success(f"å·²é€‰æ‹© {len(new_selected_ids)} ä¸ªæ¨¡å‹")


def render_model_config():
    """æ¸²æŸ“æ¨¡å‹é…ç½®åŒºåŸŸ"""
    selected_models = st.session_state.model_manager.get_selected_models()
    
    if not selected_models:
        return
    
    st.subheader("âš™ï¸ æ¨¡å‹é…ç½®")
    
    # ä¸ºæ¯ä¸ªé€‰ä¸­çš„æ¨¡å‹åˆ›å»ºé…ç½®ç•Œé¢
    for model in selected_models:
        with st.expander(f"é…ç½® {model.name} ({model.model_type.value.upper()})"):
            render_single_model_config(model)


def render_single_model_config(model):
    """æ¸²æŸ“å•ä¸ªæ¨¡å‹çš„é…ç½®ç•Œé¢"""
    config_manager = st.session_state.config_manager
    
    # è·å–å½“å‰é…ç½®
    current_config = config_manager.get_model_config(model.id)
    
    # åˆ›å»ºé…ç½®è¡¨å•
    with st.form(f"config_form_{model.id}"):
        st.write(f"**{model.name}** é…ç½®å‚æ•°")
        
        # é€šç”¨å‚æ•°
        col1, col2 = st.columns(2)
        
        with col1:
            temperature = st.slider(
                "Temperature",
                min_value=0.0,
                max_value=2.0,
                value=current_config.temperature,
                step=0.1,
                help="æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§"
            )
            
            max_tokens = st.number_input(
                "Max Tokens",
                min_value=1,
                max_value=4096,
                value=current_config.max_tokens,
                help="æœ€å¤§ç”Ÿæˆtokenæ•°é‡"
            )
        
        with col2:
            top_p = st.slider(
                "Top P",
                min_value=0.0,
                max_value=1.0,
                value=current_config.top_p,
                step=0.05,
                help="æ ¸é‡‡æ ·å‚æ•°"
            )
        
        # æ¨¡å‹ç‰¹å®šå‚æ•°
        if model.model_type == ModelType.PYTORCH:
            st.write("**PyTorch ç‰¹å®šå‚æ•°**")
            do_sample = st.checkbox(
                "Do Sample",
                value=current_config.do_sample,
                help="æ˜¯å¦ä½¿ç”¨é‡‡æ ·ç”Ÿæˆ"
            )
            
            torch_dtype = st.selectbox(
                "Torch Dtype",
                options=["auto", "float16", "float32", "bfloat16"],
                index=["auto", "float16", "float32", "bfloat16"].index(current_config.torch_dtype),
                help="PyTorchæ•°æ®ç±»å‹"
            )
            
            low_cpu_mem_usage = st.checkbox(
                "Low CPU Memory Usage",
                value=current_config.low_cpu_mem_usage,
                help="å¯ç”¨ä½CPUå†…å­˜ä½¿ç”¨æ¨¡å¼"
            )
        
        elif model.model_type == ModelType.GGUF:
            st.write("**GGUF ç‰¹å®šå‚æ•°**")
            col3, col4 = st.columns(2)
            
            with col3:
                top_k = st.number_input(
                    "Top K",
                    min_value=1,
                    max_value=100,
                    value=current_config.top_k,
                    help="Top-Ké‡‡æ ·å‚æ•°"
                )
                
                repeat_penalty = st.slider(
                    "Repeat Penalty",
                    min_value=1.0,
                    max_value=2.0,
                    value=current_config.repeat_penalty,
                    step=0.05,
                    help="é‡å¤æƒ©ç½šå‚æ•°"
                )
            
            with col4:
                n_ctx = st.number_input(
                    "Context Length",
                    min_value=512,
                    max_value=8192,
                    value=current_config.n_ctx,
                    help="ä¸Šä¸‹æ–‡é•¿åº¦"
                )
                
                use_gpu = st.checkbox(
                    "Use GPU",
                    value=current_config.use_gpu,
                    help="å¯ç”¨GPUåŠ é€Ÿ"
                )
        
        elif model.model_type == ModelType.OPENAI_API:
            st.write("**OpenAI API ç‰¹å®šå‚æ•°**")
            col5, col6 = st.columns(2)
            
            with col5:
                api_key = st.text_input(
                    "API Key",
                    value=current_config.api_key or "",
                    type="password",
                    help="OpenAI APIå¯†é’¥ï¼ˆç•™ç©ºåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰"
                )
                
                base_url = st.text_input(
                    "Base URL",
                    value=current_config.base_url or "https://ark.cn-beijing.volces.com/api/v3",
                    help="APIåŸºç¡€URL"
                )
                
                model_name = st.text_input(
                    "Model Name",
                    value=current_config.model_name or "doubao-seed-1-6-250615",
                    help="æ¨¡å‹åç§°"
                )
            
            with col6:
                presence_penalty = st.slider(
                    "Presence Penalty",
                    min_value=-2.0,
                    max_value=2.0,
                    value=current_config.presence_penalty,
                    step=0.1,
                    help="å­˜åœ¨æƒ©ç½šå‚æ•°"
                )
                
                frequency_penalty = st.slider(
                    "Frequency Penalty",
                    min_value=-2.0,
                    max_value=2.0,
                    value=current_config.frequency_penalty,
                    step=0.1,
                    help="é¢‘ç‡æƒ©ç½šå‚æ•°"
                )
                
                stream = st.checkbox(
                    "Enable Streaming",
                    value=current_config.stream,
                    help="å¯ç”¨æµå¼è¾“å‡º"
                )
        
        # ä¿å­˜æŒ‰é’®
        if st.form_submit_button("ğŸ’¾ ä¿å­˜é…ç½®"):
            try:
                # åˆ›å»ºæ–°é…ç½®
                new_config = ModelConfig(
                    temperature=temperature,
                    max_tokens=max_tokens,
                    top_p=top_p,
                    do_sample=do_sample if model.model_type == ModelType.PYTORCH else current_config.do_sample,
                    torch_dtype=torch_dtype if model.model_type == ModelType.PYTORCH else current_config.torch_dtype,
                    low_cpu_mem_usage=low_cpu_mem_usage if model.model_type == ModelType.PYTORCH else current_config.low_cpu_mem_usage,
                    top_k=top_k if model.model_type == ModelType.GGUF else current_config.top_k,
                    repeat_penalty=repeat_penalty if model.model_type == ModelType.GGUF else current_config.repeat_penalty,
                    n_ctx=n_ctx if model.model_type == ModelType.GGUF else current_config.n_ctx,
                    use_gpu=use_gpu if model.model_type == ModelType.GGUF else current_config.use_gpu,
                    api_key=api_key if model.model_type == ModelType.OPENAI_API else current_config.api_key,
                    base_url=base_url if model.model_type == ModelType.OPENAI_API else current_config.base_url,
                    model_name=model_name if model.model_type == ModelType.OPENAI_API else current_config.model_name,
                    stream=stream if model.model_type == ModelType.OPENAI_API else current_config.stream,
                    presence_penalty=presence_penalty if model.model_type == ModelType.OPENAI_API else current_config.presence_penalty,
                    frequency_penalty=frequency_penalty if model.model_type == ModelType.OPENAI_API else current_config.frequency_penalty,
                )
                
                # éªŒè¯å¹¶ä¿å­˜é…ç½®
                config_manager.save_model_config(model.id, new_config, model.model_type)
                st.success(f"âœ… {model.name} é…ç½®å·²ä¿å­˜")
                
            except ValidationError as e:
                st.error(f"é…ç½®éªŒè¯å¤±è´¥: {str(e)}")
            except Exception as e:
                st.error(f"ä¿å­˜é…ç½®å¤±è´¥: {str(e)}")


def render_prompt_input():
    """æ¸²æŸ“æç¤ºè¯è¾“å…¥åŒºåŸŸ"""
    st.subheader("ğŸ’¬ æç¤ºè¯è¾“å…¥")
    
    # è¾“å‡ºæ¨¡å¼é€‰æ‹©
    col_mode1, col_mode2 = st.columns(2)
    with col_mode1:
        output_mode = st.radio(
            "è¾“å‡ºæ¨¡å¼",
            options=["æµå¼è¾“å‡º", "å®Œæ•´è¾“å‡º"],
            index=0,
            help="æµå¼è¾“å‡ºï¼šå®æ—¶æ˜¾ç¤ºç”Ÿæˆè¿‡ç¨‹ï¼›å®Œæ•´è¾“å‡ºï¼šç­‰å¾…ç”Ÿæˆå®Œæˆåä¸€æ¬¡æ€§æ˜¾ç¤º"
        )
    
    with col_mode2:
        if output_mode == "å®Œæ•´è¾“å‡º":
            st.info("ğŸ’¡ å®Œæ•´è¾“å‡ºæ¨¡å¼å°†ä¼˜åŒ–æ€§èƒ½ï¼Œå‡å°‘èµ„æºæ¶ˆè€—")
        else:
            st.info("ğŸ’¡ æµå¼è¾“å‡ºæ¨¡å¼å¯å®æ—¶æŸ¥çœ‹ç”Ÿæˆè¿›åº¦")
    
    # å­˜å‚¨è¾“å‡ºæ¨¡å¼åˆ°ä¼šè¯çŠ¶æ€
    st.session_state.streaming_mode = (output_mode == "æµå¼è¾“å‡º")
    
    # æç¤ºè¯è¾“å…¥
    prompt = st.text_area(
        "è¾“å…¥æ‚¨çš„æç¤ºè¯",
        height=150,
        placeholder="è¯·è¾“å…¥æ‚¨æƒ³è¦å„ä¸ªæ¨¡å‹å›ç­”çš„é—®é¢˜æˆ–å®Œæˆçš„ä»»åŠ¡...",
        help="è¾“å…¥æç¤ºè¯ï¼Œæ‰€æœ‰é€‰ä¸­çš„æ¨¡å‹å°†åŸºäºæ­¤æç¤ºè¯ç”Ÿæˆå›ç­”"
    )
    
    # æ¯”è¾ƒæ§åˆ¶æŒ‰é’®
    col1, col2, col3, col4 = st.columns([2, 1, 1, 1])
    
    with col1:
        start_comparison = st.button(
            "ğŸš€ å¼€å§‹æ¯”è¾ƒ",
            disabled=not prompt.strip() or not st.session_state.model_manager.get_selected_models() or st.session_state.comparison_running,
            use_container_width=True,
            type="primary",
            help="å¼€å§‹æ¨¡å‹æ¯”è¾ƒ (Ctrl+Enter)"
        )
    
    with col2:
        if st.button("â¹ï¸ åœæ­¢", disabled=not st.session_state.comparison_running, use_container_width=True, help="åœæ­¢å½“å‰æ¯”è¾ƒ"):
            if show_operation_confirmation("åœæ­¢æ¯”è¾ƒ", "ç¡®å®šè¦åœæ­¢å½“å‰çš„æ¨¡å‹æ¯”è¾ƒå—ï¼Ÿ"):
                st.session_state.comparison_running = False
                show_toast_notification("æ¯”è¾ƒå·²åœæ­¢", NotificationType.INFO)
                st.rerun()
    
    with col3:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºç»“æœ", use_container_width=True, help="æ¸…ç©ºæ¯”è¾ƒç»“æœ"):
            if show_operation_confirmation("æ¸…ç©ºç»“æœ", "ç¡®å®šè¦æ¸…ç©ºæ‰€æœ‰æ¯”è¾ƒç»“æœå—ï¼Ÿæ­¤æ“ä½œä¸å¯æ’¤é”€ã€‚", danger=True):
                save_operation_state("æ¸…ç©ºç»“æœå‰")
                st.session_state.comparison_results = {}
                show_toast_notification("ç»“æœå·²æ¸…ç©º", NotificationType.INFO)
                st.rerun()
    
    with col4:
        # æ·»åŠ æ’¤é”€/é‡åšæŒ‰é’®
        render_undo_redo_controls()
    
    # å­˜å‚¨å½“å‰æç¤ºè¯åˆ°ä¼šè¯çŠ¶æ€
    st.session_state.current_prompt = prompt.strip()
    
    # å¼€å§‹æ¯”è¾ƒ
    if start_comparison:
        start_model_comparison(prompt.strip(), st.session_state.get('streaming_mode', True))
    
    return prompt


def start_model_comparison(prompt: str, streaming: bool = True):
    """å¼€å§‹æ¨¡å‹æ¯”è¾ƒ"""
    selected_models = st.session_state.model_manager.get_selected_models()
    
    if not selected_models:
        show_toast_notification("è¯·å…ˆé€‰æ‹©è¦æ¯”è¾ƒçš„æ¨¡å‹", NotificationType.ERROR)
        return
    
    # ä¿å­˜æ“ä½œçŠ¶æ€
    save_operation_state("å¼€å§‹æ¯”è¾ƒå‰")
    
    st.session_state.comparison_running = True
    st.session_state.comparison_results = {}
    st.session_state.streaming_mode = streaming
    
    # åˆå§‹åŒ–è¾“å‡ºå®¹å™¨
    initialize_output_containers(selected_models)
    
    # æ˜¾ç¤ºå¢å¼ºçš„è¿›åº¦æŒ‡ç¤ºå™¨
    mode_text = "æµå¼" if streaming else "å®Œæ•´"
    show_enhanced_progress(
        progress_id="model_comparison",
        title=f"æ¨¡å‹æ¯”è¾ƒ ({mode_text}æ¨¡å¼)",
        current=0,
        total=len(selected_models),
        message=f"å¼€å§‹{mode_text}æ¯”è¾ƒ {len(selected_models)} ä¸ªæ¨¡å‹"
    )
    
    show_toast_notification(f"å¼€å§‹{mode_text}æ¯”è¾ƒ {len(selected_models)} ä¸ªæ¨¡å‹", NotificationType.INFO)
    
    # æ‰§è¡ŒçœŸå®çš„æ¨¡å‹æ¨ç†
    execute_real_model_comparison(prompt, selected_models, streaming)


def initialize_output_containers(selected_models):
    """åˆå§‹åŒ–è¾“å‡ºå®¹å™¨"""
    if 'output_containers' not in st.session_state:
        st.session_state.output_containers = {}
    
    if 'output_status' not in st.session_state:
        st.session_state.output_status = {}
    
    # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºè¾“å‡ºå®¹å™¨
    for model in selected_models:
        st.session_state.output_containers[model.id] = {
            'content': "",
            'status': "pending",
            'start_time': None,
            'end_time': None,
            'token_count': 0,
            'error': None
        }
        st.session_state.output_status[model.id] = "pending"


def execute_real_model_comparison(prompt: str, selected_models, streaming: bool = True):
    """æ‰§è¡ŒçœŸå®çš„æ¨¡å‹æ¯”è¾ƒ"""
    import time
    import threading
    from queue import Queue
    
    # åˆ›å»ºç»“æœé˜Ÿåˆ—ç”¨äºçº¿ç¨‹é—´é€šä¿¡
    result_queue = Queue()
    
    # è·å–å¿…è¦çš„å¯¹è±¡å¼•ç”¨
    inference_engine = st.session_state.inference_engine
    
    def progress_callback(task_id: str, model_id: str, status: str):
        """è¿›åº¦å›è°ƒå‡½æ•° - é€šè¿‡é˜Ÿåˆ—ä¼ é€’çŠ¶æ€æ›´æ–°"""
        try:
            result_queue.put({
                'type': 'progress',
                'model_id': model_id,
                'status': status,
                'timestamp': time.time()
            })
        except Exception as e:
            logger.error(f"è¿›åº¦å›è°ƒé”™è¯¯: {str(e)}")
    
    def run_inference():
        """åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œæ¨ç†"""
        try:
            # ä¸ºæ¯ä¸ªæ¨¡å‹å‘é€åˆå§‹çŠ¶æ€
            for model in selected_models:
                result_queue.put({
                    'type': 'init',
                    'model_id': model.id,
                    'status': 'pending',
                    'timestamp': time.time()
                })
            
            # æ ¹æ®æ¨¡å¼é€‰æ‹©æ¨ç†æ–¹æ³•
            if streaming:
                inference_method = inference_engine.run_inference
            else:
                inference_method = inference_engine.run_non_streaming_inference
            
            # æ‰§è¡Œæ¨ç†
            for result in inference_method(prompt, selected_models, progress_callback):
                # é€šè¿‡é˜Ÿåˆ—å‘é€ç»“æœæ›´æ–°
                model_info = next((m for m in selected_models if m.id == result.model_id), None)
                if model_info:
                    result_queue.put({
                        'type': 'result',
                        'model_id': result.model_id,
                        'model_name': model_info.name,
                        'model_type': model_info.model_type.value,
                        'content': result.content,
                        'error': result.error,
                        'is_complete': result.is_complete,
                        'streaming': streaming,
                        'stats': {
                            'start_time': result.stats.start_time if result.stats else time.time(),
                            'end_time': result.stats.end_time if result.stats else time.time(),
                            'token_count': result.stats.token_count if result.stats else 0,
                            'duration': (result.stats.end_time - result.stats.start_time) if result.stats and result.stats.end_time else 0,
                            'tokens_per_second': result.stats.tokens_per_second if result.stats else 0
                        },
                        'timestamp': time.time()
                    })
        
        except Exception as e:
            logger.error(f"æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            # ä¸ºæ‰€æœ‰æ¨¡å‹å‘é€é”™è¯¯çŠ¶æ€
            for model in selected_models:
                result_queue.put({
                    'type': 'error',
                    'model_id': model.id,
                    'model_name': model.name,
                    'model_type': model.model_type.value,
                    'error': str(e),
                    'timestamp': time.time()
                })
        
        finally:
            # å‘é€å®Œæˆä¿¡å·
            result_queue.put({
                'type': 'finished',
                'timestamp': time.time()
            })
    
    # åœ¨åå°çº¿ç¨‹ä¸­å¯åŠ¨æ¨ç†
    inference_thread = threading.Thread(target=run_inference, daemon=True)
    inference_thread.start()
    
    # å­˜å‚¨çº¿ç¨‹å¼•ç”¨å’Œç»“æœé˜Ÿåˆ—
    st.session_state.inference_thread = inference_thread
    st.session_state.result_queue = result_queue
    
    # ç«‹å³å¤„ç†ä¸€æ¬¡é˜Ÿåˆ—ä¸­çš„ç»“æœ
    process_inference_results()


def process_inference_results():
    """å¤„ç†æ¨ç†ç»“æœé˜Ÿåˆ—ä¸­çš„æ›´æ–°"""
    if 'result_queue' not in st.session_state:
        return
    
    result_queue = st.session_state.result_queue
    updates_processed = 0
    streaming_updates = 0
    
    # å¤„ç†é˜Ÿåˆ—ä¸­çš„æ‰€æœ‰ç»“æœï¼Œå¯¹æµå¼æ›´æ–°ç»™äºˆæ›´é«˜ä¼˜å…ˆçº§
    while not result_queue.empty() and updates_processed < 100:  # å¢åŠ å¤„ç†æ•°é‡ä»¥æ”¯æŒæ›´é¢‘ç¹çš„æµå¼æ›´æ–°
        try:
            update = result_queue.get_nowait()
            updates_processed += 1
            
            if update['type'] == 'init':
                # åˆå§‹åŒ–æ¨¡å‹çŠ¶æ€
                model_id = update['model_id']
                if 'output_containers' not in st.session_state:
                    st.session_state.output_containers = {}
                
                st.session_state.output_containers[model_id] = {
                    'content': "",
                    'status': update['status'],
                    'start_time': update['timestamp'],
                    'end_time': None,
                    'token_count': 0,
                    'error': None
                }
            
            elif update['type'] == 'progress':
                # æ›´æ–°è¿›åº¦çŠ¶æ€
                model_id = update['model_id']
                status = update['status']
                
                if model_id in st.session_state.output_containers:
                    container = st.session_state.output_containers[model_id]
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æµå¼æ›´æ–°
                    if status.startswith("streaming_update:") or status.startswith("streaming_final:"):
                        streaming_updates += 1
                        # è§£ææµå¼æ›´æ–°: streaming_update:content:token_count æˆ– streaming_final:content:token_count
                        parts = status.split(":", 2)
                        if len(parts) >= 3:
                            streaming_content = parts[1]
                            token_count = int(parts[2]) if parts[2].isdigit() else 0
                            is_final = status.startswith("streaming_final:")
                            
                            # æ›´æ–°æµå¼å†…å®¹
                            container['content'] = streaming_content
                            container['token_count'] = token_count
                            container['status'] = 'completed' if is_final else 'streaming'
                            
                            # åŒæ—¶æ›´æ–°æ¯”è¾ƒç»“æœä»¥ä¾¿å®æ—¶æ˜¾ç¤º
                            if model_id not in st.session_state.comparison_results:
                                st.session_state.comparison_results[model_id] = {}
                            
                            # è®¡ç®—å½“å‰é€Ÿåº¦
                            start_time = container.get('start_time', time.time())
                            current_time = time.time()
                            duration = current_time - start_time
                            tokens_per_second = token_count / duration if duration > 0 else 0
                            
                            st.session_state.comparison_results[model_id].update({
                                'content': streaming_content,
                                'status': 'completed' if is_final else 'streaming',
                                'stats': {
                                    'token_count': token_count,
                                    'start_time': start_time,
                                    'end_time': current_time if is_final else None,
                                    'duration': duration,
                                    'tokens_per_second': tokens_per_second
                                }
                            })
                    elif "åŠ è½½" in status:
                        container['status'] = 'loading'
                    elif "ç”Ÿæˆ" in status or "è¿è¡Œ" in status:
                        container['status'] = 'running'
                    elif "å®Œæˆ" in status:
                        container['status'] = 'completed'
                    elif "é”™è¯¯" in status or "å¤±è´¥" in status:
                        container['status'] = 'error'
            
            elif update['type'] == 'result':
                # æ›´æ–°æ¨ç†ç»“æœ
                model_id = update['model_id']
                
                # æ›´æ–°è¾“å‡ºå®¹å™¨
                if model_id in st.session_state.output_containers:
                    container = st.session_state.output_containers[model_id]
                    container['content'] = update['content']
                    container['end_time'] = update['timestamp']
                    container['token_count'] = update['stats']['token_count']
                    
                    if update['error']:
                        container['status'] = 'error'
                        container['error'] = update['error']
                    else:
                        container['status'] = 'completed' if update['is_complete'] else 'running'
                
                # æ›´æ–°æ¯”è¾ƒç»“æœ
                result_status = 'completed' if update['is_complete'] else 'error' if update['error'] else 'running'
                
                # å¦‚æœä¹‹å‰æ˜¯æµå¼çŠ¶æ€ä¸”ç°åœ¨å®Œæˆï¼Œç¡®ä¿çŠ¶æ€æ­£ç¡®è½¬æ¢
                if model_id in st.session_state.comparison_results:
                    prev_status = st.session_state.comparison_results[model_id].get('status')
                    if prev_status == 'streaming' and update['is_complete']:
                        result_status = 'completed'
                
                st.session_state.comparison_results[model_id] = {
                    'model_name': update['model_name'],
                    'model_type': update['model_type'],
                    'status': result_status,
                    'content': update['content'],
                    'error': update['error'],
                    'stats': update['stats'],
                    'streaming': update.get('streaming', False)
                }
            
            elif update['type'] == 'error':
                # å¤„ç†é”™è¯¯
                model_id = update['model_id']
                
                if model_id in st.session_state.output_containers:
                    st.session_state.output_containers[model_id]['status'] = 'error'
                    st.session_state.output_containers[model_id]['error'] = update['error']
                
                st.session_state.comparison_results[model_id] = {
                    'model_name': update['model_name'],
                    'model_type': update['model_type'],
                    'status': 'error',
                    'content': '',
                    'error': update['error'],
                    'stats': {
                        'start_time': update['timestamp'],
                        'end_time': update['timestamp'],
                        'token_count': 0,
                        'duration': 0,
                        'tokens_per_second': 0
                    }
                }
            
            elif update['type'] == 'finished':
                # æ¨ç†å®Œæˆ
                st.session_state.comparison_running = False
                # æ¸…ç†é˜Ÿåˆ—å¼•ç”¨
                if 'result_queue' in st.session_state:
                    del st.session_state.result_queue
                break
                
        except Exception as e:
            logger.error(f"å¤„ç†æ¨ç†ç»“æœæ—¶å‡ºé”™: {str(e)}")
            break
    
    # å¦‚æœå¤„ç†äº†æ›´æ–°ï¼Œè§¦å‘é‡æ–°è¿è¡Œä»¥åˆ·æ–°UI
    if updates_processed > 0:
        # æ£€æŸ¥æ˜¯å¦æœ‰æµå¼æ›´æ–°
        has_streaming_update = any(
            container.get('status') == 'streaming' 
            for container in st.session_state.get('output_containers', {}).values()
        )
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é‡è¦çŠ¶æ€å˜åŒ–ï¼ˆå¼€å§‹ã€å®Œæˆã€é”™è¯¯ï¼‰
        has_important_update = any(
            container.get('status') in ['loading', 'running', 'completed', 'error']
            for container in st.session_state.get('output_containers', {}).values()
        )
        
        # åœ¨æµå¼æ¨¡å¼ä¸‹æ›´ç§¯æåœ°åˆ·æ–°UIä»¥å®ç°å®æ—¶æ˜¾ç¤º
        if st.session_state.get('streaming_mode', True):
            # æµå¼æ¨¡å¼ï¼šæœ‰ä»»ä½•æ›´æ–°éƒ½åˆ·æ–°ï¼Œç‰¹åˆ«æ˜¯æµå¼æ›´æ–°
            if streaming_updates > 0 or has_streaming_update or has_important_update:
                # æ›´æ–°æ€§èƒ½ç›‘æ§
                current_time = time.time()
                perf = st.session_state.streaming_performance
                perf['ui_refreshes'] += 1
                perf['updates_processed'] += updates_processed
                
                # è®°å½•åˆ·æ–°é—´éš”
                if perf['last_refresh_time']:
                    interval = current_time - perf['last_refresh_time']
                    perf['refresh_intervals'].append(interval)
                    # åªä¿ç•™æœ€è¿‘50æ¬¡åˆ·æ–°çš„è®°å½•
                    if len(perf['refresh_intervals']) > 50:
                        perf['refresh_intervals'] = perf['refresh_intervals'][-50:]
                
                perf['last_refresh_time'] = current_time
                st.rerun()
        elif has_important_update or st.session_state.get('comparison_running', False):
            # éæµå¼æ¨¡å¼ï¼šåªåœ¨é‡è¦çŠ¶æ€å˜åŒ–æ—¶åˆ·æ–°
            st.rerun()


def render_model_outputs():
    """æ¸²æŸ“æ¨¡å‹è¾“å‡ºæ˜¾ç¤ºåŒºåŸŸ"""
    # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•è¾“å‡ºæ•°æ®æˆ–æ­£åœ¨è¿è¡Œçš„æ¯”è¾ƒ
    has_output_data = bool(st.session_state.get('output_containers', {}))
    has_results = bool(st.session_state.comparison_results)
    is_running = st.session_state.get('comparison_running', False)
    
    if not has_output_data and not has_results and not is_running:
        return
    
    selected_models = st.session_state.model_manager.get_selected_models()
    
    if not selected_models:
        return
    
    # åˆ›å»ºå¹¶æ’çš„åˆ—å¸ƒå±€ - ä¼˜åŒ–å¸ƒå±€ä»¥æ”¯æŒæ›´å¤šæ¨¡å‹
    if len(selected_models) == 1:
        cols = st.columns(1)
    elif len(selected_models) == 2:
        cols = st.columns(2)
    elif len(selected_models) == 3:
        cols = st.columns(3)
    elif len(selected_models) == 4:
        cols = st.columns(2)  # 2x2å¸ƒå±€
    else:
        # è¶…è¿‡4ä¸ªæ¨¡å‹æ—¶ä½¿ç”¨æ»šåŠ¨å¸ƒå±€
        cols = st.columns(min(len(selected_models), 3))
    
    # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºè¾“å‡ºåŒºåŸŸ
    for i, model in enumerate(selected_models):
        if len(selected_models) <= 3:
            col_index = i
        elif len(selected_models) == 4:
            # 4ä¸ªæ¨¡å‹æ—¶ä½¿ç”¨2x2å¸ƒå±€
            col_index = i % 2
        else:
            # è¶…è¿‡4ä¸ªæ¨¡å‹æ—¶å¾ªç¯ä½¿ç”¨åˆ—
            col_index = i % len(cols)
        
        with cols[col_index]:
            # å¦‚æœæ˜¯4ä¸ªæ¨¡å‹çš„ç¬¬3ã€4ä¸ªï¼Œéœ€è¦åœ¨ç¬¬äºŒè¡Œæ˜¾ç¤º
            if len(selected_models) == 4 and i >= 2:
                if i == 2:  # ç¬¬ä¸‰ä¸ªæ¨¡å‹ï¼Œå¼€å§‹æ–°è¡Œ
                    st.markdown("---")  # åˆ†éš”çº¿
            
            render_single_model_output(model, i >= 2 if len(selected_models) == 4 else False)


def render_streaming_output(model_id: str, content: str, token_count: int, start_time: float):
    """æ¸²æŸ“ä¼˜åŒ–çš„æµå¼è¾“å‡ºæ˜¾ç¤º"""
    # å¤„ç†ç©ºå†…å®¹çš„æƒ…å†µ
    display_content = content if content.strip() else "æ­£åœ¨ç­‰å¾…æ¨¡å‹å¼€å§‹ç”Ÿæˆ..."
    
    # ä½¿ç”¨JavaScriptå®ç°è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨çš„æµå¼æ˜¾ç¤º
    streaming_html = f"""
    <div id="streaming_{model_id}" style="
        background-color: #f8f9fa; 
        padding: 15px; 
        border-radius: 8px; 
        max-height: 350px; 
        overflow-y: auto; 
        white-space: pre-wrap; 
        font-family: 'Courier New', monospace;
        border: 2px solid #28a745;
        font-size: 14px;
        line-height: 1.4;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        min-height: 100px;
    ">
{display_content}
    </div>
    <script>
        // è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
        var element = document.getElementById('streaming_{model_id}');
        if (element) {{
            element.scrollTop = element.scrollHeight;
        }}
    </script>
    """
    
    st.markdown(streaming_html, unsafe_allow_html=True)
    
    # å®æ—¶ç»Ÿè®¡ä¿¡æ¯
    if start_time:
        duration = time.time() - start_time
        speed = token_count / duration if duration > 0 and token_count > 0 else 0
        
        # ä½¿ç”¨åˆ—å¸ƒå±€æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Tokens", token_count, delta=None)
        with col2:
            st.metric("ç”¨æ—¶", f"{duration:.1f}s", delta=None)
        with col3:
            if speed > 0:
                st.metric("é€Ÿåº¦", f"{speed:.1f} t/s", delta=None)
            else:
                st.metric("é€Ÿåº¦", "è®¡ç®—ä¸­...", delta=None)


def render_single_model_output(model, is_second_row=False):
    """æ¸²æŸ“å•ä¸ªæ¨¡å‹çš„è¾“å‡ºåŒºåŸŸ"""
    # æ¨¡å‹æ ‡é¢˜å’ŒçŠ¶æ€
    if model.model_type == ModelType.PYTORCH:
        model_type_icon = "ğŸ”¥"
    elif model.model_type == ModelType.GGUF:
        model_type_icon = "âš¡"
    elif model.model_type == ModelType.OPENAI_API:
        model_type_icon = "ğŸŒ"
    else:
        model_type_icon = "â“"
    
    # è·å–è¾“å‡ºçŠ¶æ€ - ä¼˜å…ˆä½¿ç”¨output_containersä¸­çš„æ•°æ®ï¼Œå› ä¸ºå®ƒæ›´å®æ—¶
    output_data = st.session_state.output_containers.get(model.id, {})
    result_data = st.session_state.comparison_results.get(model.id, {})
    
    # ä¼˜å…ˆä½¿ç”¨output_containersä¸­çš„çŠ¶æ€å’Œå†…å®¹ï¼Œå› ä¸ºå®ƒä»¬æ›´å®æ—¶
    status = output_data.get('status', result_data.get('status', 'pending'))
    content = output_data.get('content', result_data.get('content', ''))
    error = output_data.get('error') or result_data.get('error')
    
    # çŠ¶æ€æŒ‡ç¤ºå™¨
    status_icons = {
        'pending': 'â³',
        'loading': 'ğŸ“¥',
        'running': 'ğŸ”„',
        'streaming': 'ğŸ“',
        'completed': 'âœ…',
        'error': 'âŒ'
    }
    
    status_colors = {
        'pending': 'orange',
        'loading': 'blue',
        'running': 'blue',
        'streaming': 'green', 
        'completed': 'green',
        'error': 'red'
    }
    
    status_icon = status_icons.get(status, 'â“')
    status_color = status_colors.get(status, 'gray')
    
    # æ¨¡å‹æ ‡é¢˜ - ä¼˜åŒ–æ ·å¼ï¼Œæ·»åŠ å®æ—¶çŠ¶æ€æ˜¾ç¤º
    token_count = output_data.get('token_count', result_data.get('stats', {}).get('token_count', 0))
    status_text = status
    if status == 'streaming' and token_count > 0:
        status_text = f"æµå¼ç”Ÿæˆä¸­ ({token_count} tokens)"
    elif status == 'running':
        status_text = "æ­£åœ¨ç”Ÿæˆ"
    
    st.markdown(f"""
    <div style="
        border: 2px solid {status_color}; 
        border-radius: 10px; 
        padding: 12px; 
        margin-bottom: 15px;
        background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    ">
        <h4 style="margin: 0; color: #333;">{status_icon} {model_type_icon} {model.name}</h4>
        <small style="color: #666;">{model.model_type.value.upper()} | {status_text}</small>
    </div>
    """, unsafe_allow_html=True)
    
    # è¿›åº¦æŒ‡ç¤ºå™¨ - ä¼˜åŒ–æ˜¾ç¤º
    if status in ['running', 'streaming', 'loading']:
        if status == 'streaming' and token_count > 0:
            # æµå¼è¾“å‡ºæ—¶æ˜¾ç¤ºåŠ¨æ€è¿›åº¦æ¡
            progress_value = min(token_count / 200, 1.0)  # å‡è®¾200ä¸ªtokenä¸ºæ»¡è¿›åº¦
            st.progress(progress_value)
        elif status == 'loading':
            # åŠ è½½æ—¶æ˜¾ç¤ºä¸ç¡®å®šè¿›åº¦æ¡
            st.progress(0.3)
        else:
            # è¿è¡Œæ—¶æ˜¾ç¤ºä¸­ç­‰è¿›åº¦
            st.progress(0.5)
    
    # è¾“å‡ºå†…å®¹åŒºåŸŸ - æ”¹è¿›é€»è¾‘ä»¥æ”¯æŒå®æ—¶æ˜¾ç¤º
    if error:
        st.error(f"âŒ é”™è¯¯: {error}")
    elif content or status in ['streaming', 'running']:
        # å³ä½¿å†…å®¹ä¸ºç©ºï¼Œå¦‚æœçŠ¶æ€æ˜¯streamingæˆ–runningï¼Œä¹Ÿæ˜¾ç¤ºå ä½ç¬¦
        if status == 'streaming':
            # ä½¿ç”¨ä¼˜åŒ–çš„æµå¼è¾“å‡ºç»„ä»¶
            start_time = output_data.get('start_time')
            if content:
                render_streaming_output(model.id, content, token_count, start_time)
            else:
                # æµå¼çŠ¶æ€ä½†è¿˜æ²¡æœ‰å†…å®¹æ—¶æ˜¾ç¤ºç­‰å¾…æç¤º
                st.info("ğŸ“ å‡†å¤‡å¼€å§‹æµå¼ç”Ÿæˆ...")
        elif content:
            # æœ‰å†…å®¹æ—¶æ ¹æ®çŠ¶æ€é€‰æ‹©æ˜¾ç¤ºæ–¹å¼
            if status == 'completed':
                # å®Œæˆåä½¿ç”¨å¢å¼ºçš„é«˜çº§æ–‡æœ¬æŸ¥çœ‹å™¨
                container_key = f"main_output_{model.id}_{hash(content[:50])}"
                render_advanced_text_viewer(
                    content=content,
                    title="æ¨¡å‹è¾“å‡º",
                    container_key=container_key,
                    enable_line_numbers=True,
                    enable_word_wrap=True,
                    enable_syntax_highlighting=True,
                    language="markdown",
                    max_height=400
                )
            else:
                # å…¶ä»–çŠ¶æ€ä¸‹æ˜¾ç¤ºç®€å•æ–‡æœ¬
                st.text_area(
                    "å½“å‰è¾“å‡º",
                    value=content,
                    height=300,
                    disabled=True,
                    key=f"output_{model.id}_{len(content)}"
                )
        else:
            # æ ¹æ®çŠ¶æ€æ˜¾ç¤ºç›¸åº”çš„æç¤ºä¿¡æ¯
            if status == 'pending':
                st.info("â³ ç­‰å¾…å¼€å§‹...")
            elif status == 'loading':
                st.info("ğŸ“¥ æ­£åœ¨åŠ è½½æ¨¡å‹...")
            elif status == 'running':
                st.info("ğŸ”„ æ­£åœ¨ç”Ÿæˆä¸­...")
    else:
        # é»˜è®¤çŠ¶æ€æ˜¾ç¤º
        st.info("â³ ç­‰å¾…å¼€å§‹...")
    
    # ç»Ÿè®¡ä¿¡æ¯ - æ”¯æŒå®æ—¶ç»Ÿè®¡æ˜¾ç¤º
    stats = result_data.get('stats', {}) if result_data else {}
    
    # å¦‚æœæ˜¯æµå¼çŠ¶æ€ï¼Œæ˜¾ç¤ºå®æ—¶ç»Ÿè®¡
    if status == 'streaming' and output_data.get('start_time'):
        start_time = output_data.get('start_time')
        current_time = time.time()
        duration = current_time - start_time
        speed = token_count / duration if duration > 0 and token_count > 0 else 0
        
        with st.expander("ğŸ“Š å®æ—¶ç»Ÿè®¡", expanded=False):
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("å·²ç”Ÿæˆ", f"{token_count} tokens")
            with col2:
                st.metric("ç”¨æ—¶", f"{duration:.1f}s")
            with col3:
                st.metric("é€Ÿåº¦", f"{speed:.1f} t/s")
    
    # å¦‚æœå·²å®Œæˆï¼Œæ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    elif status == 'completed' and stats:
        with st.expander("ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡"):
            col1, col2, col3 = st.columns(3)
            
            with col1:
                duration = stats.get('duration', 0)
                st.metric("ç”¨æ—¶", f"{duration:.1f}s")
            
            with col2:
                final_token_count = stats.get('token_count', 0)
                st.metric("Tokenæ•°", final_token_count)
            
            with col3:
                if duration > 0:
                    tps = final_token_count / duration
                    st.metric("é€Ÿåº¦", f"{tps:.1f} t/s")
                else:
                    st.metric("é€Ÿåº¦", "N/A")


def render_comparison_status():
    """æ¸²æŸ“æ¯”è¾ƒçŠ¶æ€å’Œè¿›åº¦"""
    if st.session_state.comparison_running:
        st.subheader("ğŸ”„ æ¯”è¾ƒè¿›è¡Œä¸­")
        
        # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
        selected_models = st.session_state.model_manager.get_selected_models()
        completed_count = len([r for r in st.session_state.comparison_results.values() 
                             if r.get('status') == 'completed'])
        
        # ä½¿ç”¨å¢å¼ºçš„è¿›åº¦æŒ‡ç¤ºå™¨
        show_enhanced_progress(
            progress_id="main_comparison",
            title="æ¨¡å‹æ¯”è¾ƒè¿›åº¦",
            current=completed_count,
            total=len(selected_models),
            message=f"æ­£åœ¨å¤„ç†ç¬¬ {completed_count + 1} ä¸ªæ¨¡å‹" if completed_count < len(selected_models) else "å³å°†å®Œæˆ",
            show_eta=True
        )
        
        # æ˜¾ç¤ºçŠ¶æ€æŒ‡ç¤ºå™¨
        render_status_indicator("running", f"æ­£åœ¨æ¯”è¾ƒ {len(selected_models)} ä¸ªæ¨¡å‹", show_spinner=True)
        
        # åœ¨æ¯”è¾ƒè¿›è¡Œä¸­ä¹Ÿæ˜¾ç¤ºå®æ—¶æµå¼è¾“å‡º
        if st.session_state.get('streaming_mode', True):
            st.subheader("ğŸ“Š å®æ—¶æ¨¡å‹è¾“å‡º")
            render_model_outputs()
    
    # æ— è®ºæ˜¯å¦åœ¨è¿è¡Œï¼Œåªè¦æœ‰ç»“æœå°±æ˜¾ç¤ºï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰
    if st.session_state.comparison_results:
        if not st.session_state.comparison_running:
            st.subheader("ğŸ“Š æ¯”è¾ƒç»“æœæ‘˜è¦")
            
            # æ˜¾ç¤ºåŠ è½½çš„ä¼šè¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if 'loaded_session_info' in st.session_state:
                session_info = st.session_state.loaded_session_info
                st.info(f"ğŸ“‚ å·²åŠ è½½ä¼šè¯: {session_info.get('name', 'æœªå‘½å')} ({session_info.get('created_at', '')[:19]})")
            
            # æ˜¾ç¤ºç»“æœç»Ÿè®¡
            total_models = len(st.session_state.comparison_results)
            completed_models = len([r for r in st.session_state.comparison_results.values() 
                                   if r.get('status') == 'completed'])
            error_models = len([r for r in st.session_state.comparison_results.values() 
                               if r.get('error')])
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ€»æ¨¡å‹æ•°", total_models)
            with col2:
                st.metric("æˆåŠŸå®Œæˆ", completed_models)
            with col3:
                st.metric("å‡ºç°é”™è¯¯", error_models)
            
            # æ¸²æŸ“æ¨¡å‹è¾“å‡º
            render_model_outputs()
            
            # æ·»åŠ å¯¼å‡ºå’Œä¼šè¯ç®¡ç†åŠŸèƒ½
            st.divider()
            
            # åˆ›å»ºé€‰é¡¹å¡
            export_tab, session_tab = st.tabs(["ğŸ“¤ å¯¼å‡ºç»“æœ", "ğŸ’¾ ä¼šè¯ç®¡ç†"])
            
            with export_tab:
                from multi_llm_comparator.ui.components import render_export_options
                render_export_options(st.session_state.comparison_results)
            
            with session_tab:
                from multi_llm_comparator.ui.components import render_session_management
                
                # è·å–å½“å‰æç¤ºè¯å’Œæ¨¡å‹ä¿¡æ¯
                current_prompt = st.session_state.get('current_prompt', '')
                selected_models = st.session_state.model_manager.get_selected_models()
                models_info = [
                    {
                        'id': model.id,
                        'name': model.name,
                        'type': model.model_type.value,
                        'path': model.path,
                        'size': model.size
                    }
                    for model in selected_models
                ]
                
                render_session_management(
                    st.session_state.comparison_results,
                    current_prompt,
                    models_info
                )
        elif not st.session_state.get('streaming_mode', True):
            # éæµå¼æ¨¡å¼ä¸‹ï¼Œå³ä½¿åœ¨è¿è¡Œä¸­ä¹Ÿä¸æ˜¾ç¤ºè¾“å‡ºï¼Œåªåœ¨å®Œæˆåæ˜¾ç¤º
            pass


def main():
    """ä¸»åº”ç”¨ç¨‹åºå…¥å£ç‚¹"""
    st.set_page_config(
        page_title="å¤šLLMæ¨¡å‹æ¯”è¾ƒå™¨",
        page_icon="ğŸ¤–",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # æ·»åŠ CSSä¼˜åŒ–ä»¥æé«˜æµå¼è¾“å‡ºæ€§èƒ½
    st.markdown("""
    <style>
    /* ä¼˜åŒ–æµå¼è¾“å‡ºæ˜¾ç¤ºæ€§èƒ½ */
    .streaming-output {
        font-family: 'Courier New', monospace;
        background-color: #f8f9fa;
        border: 2px solid #28a745;
        border-radius: 8px;
        padding: 15px;
        max-height: 350px;
        overflow-y: auto;
        white-space: pre-wrap;
        font-size: 14px;
        line-height: 1.4;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        scroll-behavior: smooth;
    }
    
    /* ä¼˜åŒ–æ¨¡å‹æ ‡é¢˜æ ·å¼ */
    .model-header {
        background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
        border-radius: 10px;
        padding: 12px;
        margin-bottom: 15px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    
    /* å‡å°‘ä¸å¿…è¦çš„åŠ¨ç”»ä»¥æé«˜æ€§èƒ½ */
    .stProgress > div > div > div > div {
        transition: width 0.1s ease-in-out;
    }
    
    /* ä¼˜åŒ–æ»šåŠ¨æ¡æ ·å¼ */
    .streaming-output::-webkit-scrollbar {
        width: 8px;
    }
    
    .streaming-output::-webkit-scrollbar-track {
        background: #f1f1f1;
        border-radius: 4px;
    }
    
    .streaming-output::-webkit-scrollbar-thumb {
        background: #888;
        border-radius: 4px;
    }
    
    .streaming-output::-webkit-scrollbar-thumb:hover {
        background: #555;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    initialize_session_state()
    
    # å¤„ç†æ¨ç†ç»“æœæ›´æ–°ï¼ˆå¦‚æœæœ‰æ­£åœ¨è¿›è¡Œçš„æ¨ç†ï¼‰
    if st.session_state.get('comparison_running', False) or 'result_queue' in st.session_state:
        process_inference_results()
    
    # ä¸»æ ‡é¢˜
    st.title("ğŸ¤– å¤šLLMæ¨¡å‹æ¯”è¾ƒå™¨")
    st.markdown("æœ¬åœ°éƒ¨ç½²çš„å¤šæ¨¡å‹æ¯”è¾ƒå·¥å…·")
    
    # æ¸²æŸ“ä¾§è¾¹æ 
    render_sidebar()
    
    # æ·»åŠ ä¸»è¦å†…å®¹æ ‡è¯†ç¬¦ï¼ˆç”¨äºæ— éšœç¢åŠŸèƒ½ï¼‰
    st.markdown('<div id="main-content"></div>', unsafe_allow_html=True)
    
    # æ˜¾ç¤ºé”®ç›˜å¿«æ·é”®å¸®åŠ©
    keyboard_manager.render_shortcuts_help()
    
    # æ˜¾ç¤ºäº¤äº’å¼æ•™ç¨‹ï¼ˆä»…åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶ï¼‰
    if st.session_state.get('show_tutorial', True):
        create_interactive_tutorial()
        if st.session_state.get('tutorial_step', 0) >= 6:  # æ•™ç¨‹å®Œæˆ
            st.session_state.show_tutorial = False
    
    # ä¸»å†…å®¹åŒºåŸŸ
    # æ¨¡å‹é€‰æ‹©
    render_model_selection()
    
    # æ¨¡å‹é…ç½®
    render_model_config()
    
    # åˆ†éš”çº¿
    st.divider()
    
    # æç¤ºè¯è¾“å…¥
    render_prompt_input()
    
    # æ¯”è¾ƒçŠ¶æ€æ˜¾ç¤º
    render_comparison_status()
    
    # å¦‚æœæœ‰æ­£åœ¨è¿›è¡Œçš„æ¨ç†ï¼Œè®¾ç½®è‡ªåŠ¨åˆ·æ–°
    if st.session_state.get('comparison_running', False):
        # æ£€æŸ¥æ˜¯å¦æœ‰æµå¼è¾“å‡ºæ­£åœ¨è¿›è¡Œ
        has_streaming = any(
            container.get('status') == 'streaming' 
            for container in st.session_state.get('output_containers', {}).values()
        )
        
        if has_streaming and st.session_state.get('streaming_mode', True):
            # æµå¼æ¨¡å¼ä¸‹æ›´é¢‘ç¹åˆ·æ–°ä»¥å®ç°å®æ—¶æ˜¾ç¤º
            time.sleep(0.05)  # å‡å°‘å»¶è¿Ÿï¼Œæé«˜å“åº”æ€§
        else:
            # éæµå¼æ¨¡å¼ä¸‹è¾ƒæ…¢åˆ·æ–°
            time.sleep(0.3)  # ç¨å¾®æé«˜éæµå¼æ¨¡å¼çš„å“åº”æ€§
        
        st.rerun()


if __name__ == "__main__":
    main()